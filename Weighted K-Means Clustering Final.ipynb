{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38ffe37f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Weighted K Means Clustering   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25387f76",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed84d2a4",
   "metadata": {},
   "source": [
    "The Weighted K-means clustering is a new k-means clustering algorithm that can automatically weight variables based on the importance of the variables in clustering. Weighted k-means adds a new step to the basic k-means algorithm to update the variable weights based on the current partition of data. It uses a weight calculation formula that minimizes the cost function of clustering given a fixed partition of data.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c53fa7",
   "metadata": {},
   "source": [
    " ## 2. Implementation on IRIS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c6c2c7",
   "metadata": {},
   "source": [
    "Here I implement the Weighted K means clustering algorithm on the [IRIS dataset](https://archive.ics.uci.edu/ml/datasets/iris). The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant namely **Iris-setosa, Iris-versicolor** and **Iris-virginica**.   \n",
    "\n",
    "I would use the original [paper](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.652.9691&rep=rep1&type=pdf) by Joshua Zhexue Huang, Michael K. Ng, Hongqiang Rong, and Zichen Li as a reference and almost the same set of notations for the variables.   \n",
    "\n",
    "Let $X= \\{ X_{1}, \\ X_2,...,\\ X_n \\}$ be a set of $n$ objects. Object $X_i= \\{ x_{i,1}, \\ x_{i,2},...,\\ x_{i,m}\\}$ is characterized by a set of $m$ variables. We want to partition the $n$ objects into $k$ clusters that minimizes the objective function $P$ with unknown variables $U$,$Z$ and $W$ as follows: $$  P(U,Z,W)= \\sum_{l=1}^{k}\\sum_{i=1}^{n}\\sum_{j=1}^{m} u_{i,l} w_j^{\\beta} d(x_{i,j}, z_{l,j})  $$ where,  \n",
    "- $U$ is an $n \\times k$ partition matrix, $u_{i,l}$ is a binary variable, and $u_{i,l}=1$ indicates that object $i$ is allocated to cluster $l$;  \n",
    "- $Z= \\{ Z_{1}, \\ Z_2,...,\\ Z_k \\}$ is a set of $k$ vectors representing the centroids of the $k$ clusters;   \n",
    "- $d(x_{i,j}, z_{l,j}) $ is a distance measure between object $i$ and the centroid of the cluster $l$ on the $j$th variable. We define here $d(x_{i,j}, z_{l,j}) $ as : $$d(x_{i,j}, z_{l,j})= (x_{i,j}-z_{l,j})^{2}.$$   \n",
    "- $W= \\{ w_{1}, \\ w_2,...,\\ w_m \\}$ be the weights for the $m$ variables such that $\\sum_{j=1}^{m} w_{j}=1$ , $0 \\le w_j \\le 1.$   \n",
    "So to minimize the cost function $P$, we have the Weighted k-means clustering as follows:  \n",
    "\n",
    "**Weighted k-means Algorithm**    \n",
    "1. Randomly choose an initial $Z^{0}=\\{Z_{1},Z_{2},...,Z_{k}\\}$ and randomly generate a set of initial weights $W^{0}=[ w_{1}^{0},\\ w_{2}^{0},...,\\ w_{m}^{0}] \\ (\\sum_{j=1}^{m} w_{j}=1)$. Determine $U^{0}$ such that $P(U^{0},Z^{0},W^{0})$ is minimized. Set $t=0$ ;   \n",
    "2. Let $\\hat{Z}=Z^{t}$ and $\\hat{W}=W^{t}$, solve problem $P(U,\\hat{Z},\\hat{W})$ to obtain $U^{t+1}$. If $P(U^{t+1},\\hat{Z},\\hat{W})=P(U^{t},\\hat{Z},\\hat{W})$, output $(U^{t},\\hat{Z},\\hat{W})$ and stop; otherwise go to step 3;   \n",
    "3. Let $\\hat{U}=U^{t+1}$ and $\\hat{W}=W^{t}$, solve problem $P(\\hat{U},Z,\\hat{W})$ to obtain $Z^{t+1}$. If $P(\\hat{U},Z^{t+1},\\hat{W})=P(\\hat{U},Z^{t},\\hat{W})$, output $(\\hat{U},Z^{t},\\hat{W})$ and stop; otherwise go to step 4;   \n",
    "4. Let $\\hat{U}=U^{t+1}$ and $\\hat{Z}=Z^{t+1}$, solve problem $P(\\hat{U},\\hat{Z},W)$ to obtain $W^{t+1}$. If $P(\\hat{U},\\hat{Z},W^{t+1})=P(\\hat{U},\\hat{Z},W^{t})$, output $(\\hat{U},\\hat{Z},W^{t})$ and stop; otherwise, set $t=t+1$ go to step 2.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ddabf4",
   "metadata": {},
   "source": [
    "#### 2.1 Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cc7e52",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries (pandas and Numpy) and reading the dataset. I exclude the last column of our dataset (Species) and denote the new dataset (with just the first four columns) as $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ceca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22944fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "iris=pd.read_csv (\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "                  names = [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\", \"Class\" ])  #Reading the csv file\n",
    "X = iris.iloc[:,0:4]\n",
    "y = iris.iloc[:,-1]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cdf3aa",
   "metadata": {},
   "source": [
    "#### 2.2 Defining a few distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_ed(point_1, point_2,A):\n",
    "    distance = np.sum((np.dot(A,np.square(np.array(point_1) - np.array(point_2)))))\n",
    "    return distance\n",
    "\n",
    "def np_ed0(point_1, point_2):\n",
    "    array_1, array_2 = np.array(point_1), np.array(point_2)\n",
    "    distance0 = np.square(np.array(point_1)[0] - np.array(point_2)[0])\n",
    "    return distance0\n",
    "\n",
    "def np_ed1(point_1, point_2):\n",
    "    array_1, array_2 = np.array(point_1), np.array(point_2)\n",
    "    distance1 = np.square(np.array(point_1)[1] - np.array(point_2)[1])\n",
    "    return distance1\n",
    "\n",
    "def np_ed2(point_1, point_2):\n",
    "    array_1, array_2 = np.array(point_1), np.array(point_2)\n",
    "    distance2 = np.square(np.array(point_1)[2] - np.array(point_2)[2])\n",
    "    return distance2\n",
    "\n",
    "def np_ed3(point_1, point_2):\n",
    "    array_1, array_2 = np.array(point_1), np.array(point_2)\n",
    "    distance3 = np.square(np.array(point_1)[3] - np.array(point_2)[3])\n",
    "    return distance3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c3ec0",
   "metadata": {},
   "source": [
    "#### 2.3 Initializing the first centroids, weights and determining $T$ by minimizing $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa9e472",
   "metadata": {},
   "source": [
    "As the 1st step of the Weighted K Means clustering we initialize the centroid vector and randomly generate the set of initial weights. Then we determine the clusters for each data points in the first iteration.   \n",
    "**Replacing $U$ by $T$**   \n",
    "In the original paper, to denote the clusters for each data point in every iteration, the $n \\times k$ matrix $U$ was used. The elements $u_{i,l}$ of $U$ is a binary variable, and $u_{i,l}=1$ indicates that object $i$ is allocated to cluster $l$. For brevity and ease, I use a different variable $T$ which is a $(150 \\times 1)$ column vector, each element of $T$ is a number from the set $\\{1,2,...,k\\}$ and $t_{i,1}=l$ ,$(l \\in \\ \\{1,2,...,k\\})$, means that the $i$th data point belongs to the $l$th cluster. So rewriting our cost function $P$ in terms of $T$, $$  P(T,Z,W)= \\sum_{l=1}^{k}\\sum_{i=1}^{n}\\sum_{j=1}^{m} \\mathbb{1}(t_{i,1}=l) w_j^{\\beta} d(x_{i,j}, z_{l,j})  $$ where $\\mathbb{1}$ is the indicator function defined as \n",
    "$$\\begin{equation}\n",
    "\\mathbb{1}(t_{i,1}=l) =\n",
    "    \\begin{cases}\n",
    "        1 & \\text{if the $i$th data point belongs to the $l$th cluster} \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}$$\n",
    "\n",
    "For the IRIS dataset, we have the no. of data points $n=150$, the number of clusters $k=3$ and the no. of variables $m=4$. So our cost function $P$ for the IRIS dataset is  $$  P(T,Z,W)= \\sum_{l=1}^{3}\\sum_{i=1}^{150}\\sum_{j=0}^{3} \\mathbb{1}(t_{i,1}=l) w_j^{\\beta} d(x_{i,j}, z_{l,j})  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018361c",
   "metadata": {},
   "outputs": [],
   "source": [
    "M1=X.iloc[5]            # Initializing the centroids\n",
    "M2=X.iloc[62]\n",
    "M3=X.iloc[105]\n",
    "Z=[M1,M2,M3]\n",
    "B=2                      #Selecting Beta(B)\n",
    "W=np.array([[pow(0.69,B),0,0,0],[0,pow(0.1,B),0,0],[0,0,pow(0.03,B),0], [0,0,0,pow(0.18,B)]])  #Initializing the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55c6af",
   "metadata": {},
   "source": [
    "#### 2.4 Iteration for the new clusters ($T$- 2nd Step of W k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a7816",
   "metadata": {},
   "source": [
    "This is the 2nd step of the W k-means clustering. Using the $Z$ and $W$ of the previous iteration we solve for the $T$ that minimizes the cost function $P$. The following code gives us the number of data points that belong to each cluster in this iteration and the value of $P$ with $Z$, $W$ and the new $T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741ffd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T=[]\n",
    "P=0\n",
    "for i in range(150):\n",
    "    ed1 = np_ed(M1, X.iloc[i],W)\n",
    "    ed2 = np_ed(M2, X.iloc[i],W)     # Calculating the new distances and creating the new clusters \n",
    "    ed3 = np_ed(M3, X.iloc[i],W)\n",
    "    mini = min(ed1, ed2, ed3) \n",
    "    if (mini==ed1):\n",
    "          S=1\n",
    "    elif (mini==ed2):\n",
    "         S=2\n",
    "    else:\n",
    "          S=3\n",
    "    T=T+[S]    \n",
    "np.T=np.array(T)    \n",
    "\n",
    "for i in range(150):\n",
    "    P =  P + np_ed(X.iloc[i], Z[np.T[i]-1],W)\n",
    "\n",
    "    \n",
    "printmd(\"**The Matrix T :**\")\n",
    "print(np.T)\n",
    "printmd(\"**No. of data points in 1st cluster:**\") \n",
    "print(np.count_nonzero(np.T == 1))\n",
    "printmd(\"**No. of data points in 2nd cluster:**\") \n",
    "print(np.count_nonzero(np.T == 2))\n",
    "printmd(\"**No. of data points in 3rd cluster:**\")\n",
    "print(np.count_nonzero(np.T == 3))  \n",
    "printmd(\"**Centroids:**\")\n",
    "print(Z)\n",
    "printmd(\"**The Weight Matrix:**\")\n",
    "print(W)\n",
    "printmd(\"**Value of P:**\")\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc544448",
   "metadata": {},
   "source": [
    "#### 2.5 Iteration for the centroids ($Z$- 3rd Step of W k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa637d",
   "metadata": {},
   "source": [
    "This is the 3rd step of the W k-means clustering. Using the $T$ and $W$ of the previous iteration we solve for the $Z$ that minimizes the cost function $P$. Solving for $Z$, we get that $P$ is minimized when $$z_{l,j}= \\frac{\\sum_{i=1}^{150} \\mathbb{1}(t_{i,1}=l)\\ x_{i,j}}{\\sum_{i=1}^{150} \\mathbb{1}(t_{i,1}=l)}$$where $1 \\le l \\le 3$ and $0 \\le j \\le 3$.  The following code gives us the new centroids and the value of $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1=[0,0,0,0]\n",
    "Z2=[0,0,0,0]                    \n",
    "Z3=[0,0,0,0]\n",
    "P=0\n",
    "for j in range(150):\n",
    "    if (np.T[j]==1):\n",
    "        Z1=(Z1+X.iloc[j])\n",
    "    elif (np.T[j]==2):\n",
    "        Z2=(Z2+X.iloc[j])                       #Calculating the new centroids\n",
    "    else:\n",
    "        Z3=(Z3+X.iloc[j])\n",
    "M1=Z1/np.count_nonzero(np.T == 1)\n",
    "M2=Z2/np.count_nonzero(np.T == 2)\n",
    "M3=Z3/np.count_nonzero(np.T == 3)\n",
    "Z=[M1,M2,M3]    \n",
    "\n",
    "for i in range(150):\n",
    "    P=  P + np_ed(X.iloc[i], Z[np.T[i]-1],W)\n",
    "\n",
    "printmd(\"**The Matrix T :**\")\n",
    "print(np.T)\n",
    "printmd(\"**No. of data points in 1st cluster:**\") \n",
    "print(np.count_nonzero(np.T == 1))\n",
    "printmd(\"**No. of data points in 2nd cluster:**\") \n",
    "print(np.count_nonzero(np.T == 2))\n",
    "printmd(\"**No. of data points in 3rd cluster:**\")\n",
    "print(np.count_nonzero(np.T == 3))  \n",
    "printmd(\"**Centroids:**\")\n",
    "print(Z)\n",
    "printmd(\"**The Weight Matrix:**\")\n",
    "print(W)\n",
    "printmd(\"**Value of P:**\")\n",
    "print(P)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa9e02",
   "metadata": {},
   "source": [
    "#### 2.6 Iteration for the weights ($W$- 4th Step of W k-means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214e7e7",
   "metadata": {},
   "source": [
    "This is the 4th step of the W k-means clustering. Using the $T$ and $Z$ of the previous iteration we solve for the $W$ that minimizes the cost function $P$. Solving for $W$, we get that $P$ is minimized when $$w_j= \\frac{1}{\\sum_{t=0}^{3} \\left[\\frac{D_j}{D_t}\\right]^{\\frac{1}{\\beta - 1}}} \\ \\ \\ \\ \\ (j\\in \\{0,1,2,3\\} \\ \\text{and} \\ \\beta=2)$$ where $$D_t= \\sum_{l=1}^{3}\\sum_{i=1}^{150} \\mathbb{1}(t_{i,1}=l) d(x_{i,t}, z_{l,t})  $$ The following code gives us the new weights and the value of $P$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806ad03",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_0= 0\n",
    "D_1= 0\n",
    "D_2= 0\n",
    "D_3= 0\n",
    "P= 0\n",
    "for i in range(150):\n",
    "    D_0 = D_0 + np_ed0(X.iloc[i], Z[np.T[i]-1])\n",
    "    D_1 = D_1 + np_ed1(X.iloc[i], Z[np.T[i]-1])         #Calculating the D_i's\n",
    "    D_2 = D_2 + np_ed2(X.iloc[i], Z[np.T[i]-1])\n",
    "    D_3 = D_3 + np_ed3(X.iloc[i], Z[np.T[i]-1])\n",
    "    T2=  (1/(pow((D_0),1/(B-1))) + 1/(pow((D_1),1/(B-1))) + 1/(pow((D_2),1/(B-1))) + 1/(pow((D_3),1/(B-1))))\n",
    "\n",
    "\n",
    "w0= 1/((pow((D_0),1/(B-1)))*(T2))\n",
    "w1= 1/((pow((D_1),1/(B-1)))*(T2))\n",
    "w2= 1/((pow((D_2),1/(B-1)))*(T2))\n",
    "w3= 1/((pow((D_3),1/(B-1)))*(T2))\n",
    "\n",
    "W=np.array([[pow(w0,B),0,0,0],[0,pow(w1,B),0,0],[0,0,pow(w2,B),0], [0,0,0,pow(w3,B)]])\n",
    "\n",
    "for i in range(150):\n",
    "    P= P + np_ed(X.iloc[i], Z[np.T[i]-1],W)\n",
    "\n",
    "printmd(\"**The Matrix T :**\")\n",
    "print(np.T)\n",
    "printmd(\"**No. of data points in 1st cluster:**\") \n",
    "print(np.count_nonzero(np.T == 1))\n",
    "printmd(\"**No. of data points in 2nd cluster:**\") \n",
    "print(np.count_nonzero(np.T == 2))\n",
    "printmd(\"**No. of data points in 3rd cluster:**\")\n",
    "print(np.count_nonzero(np.T == 3))  \n",
    "printmd(\"**Centroids:**\")\n",
    "print(Z)\n",
    "printmd(\"**The Weight Matrix:**\")\n",
    "print(W)\n",
    "printmd(\"**Value of P:**\")\n",
    "print(P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
